{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#（1）必要なライブラリのインポートとMNISTデータセットの取得\n",
    "import tensorflow as tf\n",
    "# MNIST Data取得に向けた準備\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import  datetime as dt\n",
    "import numpy as np\n",
    "# 学習過程で生成される画像を保管するpkl形式ファイル\n",
    "import pickle as pkl\n",
    "# ハイパーパラメータを指定した活性化関数を指定する際に使用します\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 精度向上は下記パラメータを調整し行う\n",
    "# エポック数(学習回数)、EPOCHSを変更した場合はビューアプログラム(gview.py)にて\n",
    "# 定義しているEPOCHSも同じ値を指定してください。\n",
    "EPOCHS = 100\n",
    "# バッチサイズ\n",
    "BATCH_SIZE=100\n",
    "# 学習率\n",
    "LEARNING_RATE = 0.001\n",
    "# 活性化関数のハイパーパラメータ設定\n",
    "ALPHA = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#（2）生成モデル（Generator）を作る関数の定義\n",
    "def generator(randomData, alpha, reuse=False):\n",
    "      with tf.variable_scope('GAN/generator', reuse=reuse):\n",
    "        # 隠れ層\n",
    "        h1 = tf.layers.dense(randomData, 256,\n",
    "                      activation=partial(tf.nn.leaky_relu, alpha=alpha))\n",
    "        # 出力層\n",
    "        o1 = tf.layers.dense(h1, 784, activation=None)\n",
    "        # 活性化関数 tanh\n",
    "        img = tf.tanh(o1) \n",
    "\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#（3）識別モデル（Discriminator）を作る関数の定義\n",
    "def discriminator(img, alpha, reuse=False):\n",
    "    with tf.variable_scope('GAN/discriminator', reuse=reuse):\n",
    "        # 隠れ層\n",
    "        h1 = tf.layers.dense(img, 128, \n",
    "                      activation=partial(tf.nn.leaky_relu, alpha=alpha))\n",
    "        # 出力層\n",
    "        D_logits = tf.layers.dense(h1, 1, activation=None)\n",
    "        # 活性化関数\n",
    "        D = tf.nn.sigmoid(D_logits)\n",
    "\n",
    "        return D, D_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ino\n",
    "#for GeForce GTX 1080 Ti\n",
    "config = tf.ConfigProto() #ino\n",
    "config.gpu_options.allow_growth = True #ino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ./MNIST_DataSet/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ./MNIST_DataSet/train-labels-idx1-ubyte.gz\n",
      "Extracting ./MNIST_DataSet/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./MNIST_DataSet/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "16:30:46 Epoch=1/100, DLoss=0.0228, GLoss=3.8733\n",
      "16:30:47 Epoch=2/100, DLoss=0.1001, GLoss=2.8710\n",
      "16:30:49 Epoch=3/100, DLoss=0.9651, GLoss=2.7890\n",
      "16:30:50 Epoch=4/100, DLoss=0.6120, GLoss=3.5002\n",
      "16:30:51 Epoch=5/100, DLoss=0.8099, GLoss=5.8589\n",
      "16:30:53 Epoch=6/100, DLoss=1.3051, GLoss=1.9541\n",
      "16:30:54 Epoch=7/100, DLoss=3.1553, GLoss=0.8698\n",
      "16:30:56 Epoch=8/100, DLoss=1.8290, GLoss=0.7469\n",
      "16:30:57 Epoch=9/100, DLoss=1.5027, GLoss=0.8353\n",
      "16:30:58 Epoch=10/100, DLoss=1.5399, GLoss=1.0253\n",
      "16:31:00 Epoch=11/100, DLoss=1.1214, GLoss=1.5136\n",
      "16:31:01 Epoch=12/100, DLoss=0.8828, GLoss=1.0655\n",
      "16:31:02 Epoch=13/100, DLoss=1.6595, GLoss=1.3827\n",
      "16:31:04 Epoch=14/100, DLoss=0.8491, GLoss=1.4754\n",
      "16:31:06 Epoch=15/100, DLoss=2.7928, GLoss=0.4396\n",
      "16:31:07 Epoch=16/100, DLoss=1.1344, GLoss=1.2140\n",
      "16:31:09 Epoch=17/100, DLoss=1.0707, GLoss=1.0195\n",
      "16:31:11 Epoch=18/100, DLoss=1.4679, GLoss=0.8965\n",
      "16:31:12 Epoch=19/100, DLoss=1.3758, GLoss=1.0966\n",
      "16:31:13 Epoch=20/100, DLoss=1.7229, GLoss=1.0264\n",
      "16:31:15 Epoch=21/100, DLoss=2.2254, GLoss=1.7311\n",
      "16:31:16 Epoch=22/100, DLoss=1.7483, GLoss=1.0608\n",
      "16:31:18 Epoch=23/100, DLoss=1.1619, GLoss=1.2884\n",
      "16:31:19 Epoch=24/100, DLoss=1.1916, GLoss=1.5990\n",
      "16:31:21 Epoch=25/100, DLoss=1.1531, GLoss=1.5948\n",
      "16:31:22 Epoch=26/100, DLoss=2.2064, GLoss=1.1237\n",
      "16:31:24 Epoch=27/100, DLoss=0.7309, GLoss=2.2785\n",
      "16:31:25 Epoch=28/100, DLoss=1.0631, GLoss=1.5541\n",
      "16:31:26 Epoch=29/100, DLoss=1.0972, GLoss=2.0898\n",
      "16:31:28 Epoch=30/100, DLoss=1.1146, GLoss=2.2795\n",
      "16:31:29 Epoch=31/100, DLoss=0.8261, GLoss=1.8648\n",
      "16:31:30 Epoch=32/100, DLoss=1.2174, GLoss=1.6847\n",
      "16:31:31 Epoch=33/100, DLoss=0.7545, GLoss=1.7214\n",
      "16:31:33 Epoch=34/100, DLoss=0.5588, GLoss=2.2062\n",
      "16:31:34 Epoch=35/100, DLoss=1.2162, GLoss=1.7026\n",
      "16:31:35 Epoch=36/100, DLoss=0.9964, GLoss=2.4108\n",
      "16:31:37 Epoch=37/100, DLoss=1.1430, GLoss=1.5173\n",
      "16:31:38 Epoch=38/100, DLoss=0.8134, GLoss=1.9129\n",
      "16:31:39 Epoch=39/100, DLoss=1.0507, GLoss=1.5144\n",
      "16:31:40 Epoch=40/100, DLoss=1.0749, GLoss=1.0676\n",
      "16:31:42 Epoch=41/100, DLoss=0.9581, GLoss=1.3013\n",
      "16:31:44 Epoch=42/100, DLoss=1.2763, GLoss=1.4419\n",
      "16:31:45 Epoch=43/100, DLoss=0.6568, GLoss=1.9444\n",
      "16:31:47 Epoch=44/100, DLoss=1.1159, GLoss=1.1283\n",
      "16:31:48 Epoch=45/100, DLoss=0.8985, GLoss=1.5061\n",
      "16:31:50 Epoch=46/100, DLoss=1.2378, GLoss=1.3656\n",
      "16:31:51 Epoch=47/100, DLoss=1.0383, GLoss=1.6404\n",
      "16:31:52 Epoch=48/100, DLoss=1.2063, GLoss=1.4014\n",
      "16:31:53 Epoch=49/100, DLoss=1.1418, GLoss=1.3055\n",
      "16:31:55 Epoch=50/100, DLoss=0.9609, GLoss=1.5854\n",
      "16:31:56 Epoch=51/100, DLoss=0.9114, GLoss=1.7357\n",
      "16:31:58 Epoch=52/100, DLoss=1.0910, GLoss=1.3802\n",
      "16:32:00 Epoch=53/100, DLoss=1.3961, GLoss=1.5557\n",
      "16:32:01 Epoch=54/100, DLoss=0.6553, GLoss=2.0146\n",
      "16:32:02 Epoch=55/100, DLoss=0.7805, GLoss=1.6284\n",
      "16:32:03 Epoch=56/100, DLoss=1.2179, GLoss=1.5830\n",
      "16:32:05 Epoch=57/100, DLoss=1.0408, GLoss=1.3428\n",
      "16:32:06 Epoch=58/100, DLoss=1.1931, GLoss=1.4094\n",
      "16:32:08 Epoch=59/100, DLoss=1.2683, GLoss=1.3067\n",
      "16:32:10 Epoch=60/100, DLoss=1.2124, GLoss=1.2715\n",
      "16:32:11 Epoch=61/100, DLoss=1.0025, GLoss=1.6225\n",
      "16:32:13 Epoch=62/100, DLoss=1.3456, GLoss=1.0444\n",
      "16:32:15 Epoch=63/100, DLoss=0.7417, GLoss=1.6425\n",
      "16:32:16 Epoch=64/100, DLoss=1.0127, GLoss=1.3485\n",
      "16:32:17 Epoch=65/100, DLoss=0.7493, GLoss=1.7038\n",
      "16:32:18 Epoch=66/100, DLoss=1.1790, GLoss=1.2033\n",
      "16:32:20 Epoch=67/100, DLoss=1.0274, GLoss=1.3737\n",
      "16:32:21 Epoch=68/100, DLoss=1.0387, GLoss=1.7478\n",
      "16:32:22 Epoch=69/100, DLoss=1.0232, GLoss=1.3380\n",
      "16:32:24 Epoch=70/100, DLoss=1.1035, GLoss=1.2859\n",
      "16:32:26 Epoch=71/100, DLoss=1.0261, GLoss=1.5181\n",
      "16:32:28 Epoch=72/100, DLoss=1.2003, GLoss=1.2412\n",
      "16:32:30 Epoch=73/100, DLoss=0.9925, GLoss=1.3741\n",
      "16:32:32 Epoch=74/100, DLoss=1.0897, GLoss=1.6912\n",
      "16:32:33 Epoch=75/100, DLoss=1.0724, GLoss=1.5323\n",
      "16:32:35 Epoch=76/100, DLoss=1.0698, GLoss=1.3324\n",
      "16:32:36 Epoch=77/100, DLoss=0.9930, GLoss=1.4825\n",
      "16:32:38 Epoch=78/100, DLoss=0.8742, GLoss=1.3849\n",
      "16:32:40 Epoch=79/100, DLoss=1.1546, GLoss=1.2866\n",
      "16:32:41 Epoch=80/100, DLoss=0.8140, GLoss=1.5668\n",
      "16:32:42 Epoch=81/100, DLoss=1.0468, GLoss=1.2135\n",
      "16:32:44 Epoch=82/100, DLoss=1.0803, GLoss=1.1853\n",
      "16:32:45 Epoch=83/100, DLoss=1.1578, GLoss=1.1550\n",
      "16:32:47 Epoch=84/100, DLoss=1.0781, GLoss=1.2680\n",
      "16:32:49 Epoch=85/100, DLoss=1.0377, GLoss=1.4921\n",
      "16:32:50 Epoch=86/100, DLoss=1.0022, GLoss=1.4067\n",
      "16:32:52 Epoch=87/100, DLoss=1.0062, GLoss=1.5958\n",
      "16:32:54 Epoch=88/100, DLoss=1.0116, GLoss=1.3155\n",
      "16:32:55 Epoch=89/100, DLoss=0.7829, GLoss=1.4766\n",
      "16:32:56 Epoch=90/100, DLoss=0.9465, GLoss=1.3491\n",
      "16:32:57 Epoch=91/100, DLoss=1.1403, GLoss=1.3752\n",
      "16:32:59 Epoch=92/100, DLoss=1.1497, GLoss=1.2323\n",
      "16:33:00 Epoch=93/100, DLoss=1.2537, GLoss=1.2171\n",
      "16:33:01 Epoch=94/100, DLoss=0.9372, GLoss=1.3530\n",
      "16:33:03 Epoch=95/100, DLoss=0.9657, GLoss=1.3318\n",
      "16:33:04 Epoch=96/100, DLoss=1.0177, GLoss=1.5291\n",
      "16:33:05 Epoch=97/100, DLoss=0.9698, GLoss=1.8183\n",
      "16:33:07 Epoch=98/100, DLoss=1.2939, GLoss=1.2973\n",
      "16:33:09 Epoch=99/100, DLoss=1.0859, GLoss=1.3502\n",
      "16:33:11 Epoch=100/100, DLoss=1.0847, GLoss=1.5357\n",
      "開始: 16:30:42、終了:16:33:11、処理時間:0:02:29\n",
      "counter=55000\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # 処理開始時刻の取得\n",
    "    tstamp_s = dt.datetime.now().strftime(\"%H:%M:%S\")\n",
    "    # MNISTデータセットの取得\n",
    "    mnist = input_data.read_data_sets('./MNIST_DataSet')\n",
    "\n",
    "    # プレースホルダー\n",
    "    # 本物画像データ784次元(28x28ピクセル)をバッチサイズ分保管するプレースホルダ(ph_realData)を準備する\n",
    "    ph_realData = tf.placeholder(tf.float32, (BATCH_SIZE, 784))\n",
    "    # 100次元の一様性乱数を保管するプレースホルダ(ph_randomData)を準備する、\n",
    "    # 確保するサイズとして、学習時はバッチサイズの100件、各エポックでの画像生成は25件と、\n",
    "    # 動的に変わるため、Noneを指定し実行時にサイズを決定するようにする\n",
    "    ph_randomData = tf.placeholder(tf.float32, (None,100))\n",
    "\n",
    "    # 一様性乱数を与えて画像を生成\n",
    "    gimage = generator(ph_randomData, ALPHA)\n",
    "    # 本物の画像を与えて判定結果を取得\n",
    "    real_D, real_D_logits = discriminator(ph_realData, ALPHA)\n",
    "    # 生成画像を与えて判定結果を取得\n",
    "    fake_D, fake_D_logits = discriminator(gimage, ALPHA, reuse = True)\n",
    "\n",
    "    #（3）損失関数の実装\n",
    "    # 本物画像（ラベル＝１）との誤差をクロスエントロピーの平均として取得\n",
    "    d_real_xentropy = tf.nn.sigmoid_cross_entropy_with_logits(logits=real_D_logits, labels=tf.ones_like(real_D))\n",
    "    loss_real = tf.reduce_mean(d_real_xentropy)\n",
    "    # 生成画像（ラベル=0)との誤差をクロスエントロピーの平均として取得\n",
    "    d_fake_xentropy = tf.nn.sigmoid_cross_entropy_with_logits(logits=fake_D_logits, labels=tf.zeros_like(fake_D))\n",
    "    loss_fake = tf.reduce_mean(d_fake_xentropy)\n",
    "    # Discriminatorの誤差は、本物画像、生成画像における誤差を合計した値となる\n",
    "    d_loss = loss_real + loss_fake\n",
    "    # Generatorの誤差を取得\n",
    "    g_xentropy = tf.nn.sigmoid_cross_entropy_with_logits(logits=fake_D_logits, labels=tf.ones_like(fake_D))\n",
    "    g_loss = tf.reduce_mean(g_xentropy)\n",
    "\n",
    "    # 学習によって最適化を行うパラメータ(重み、バイアス)をtf.trainable_variablesから\n",
    "    # 一括して取得する。取得の際にDiscriminator用(d_training_parameter)、\n",
    "    # Generator用(g_training_parameter)と取り分けて、それぞれのネットワークを\n",
    "    # 最適化していく必要があるため、ネットワーク定義時に指定したスコープの名前を指定して、\n",
    "    # 取り分けを行う。\n",
    "    # discriminatorの最適化を行う学習パラメータを取得（一旦、trainVarに取り分けてから格納）\n",
    "    d_training_parameter = [trainVar for trainVar in tf.trainable_variables()\n",
    "                          if 'GAN/discriminator/' in trainVar.name]\n",
    "    # generatorの最適化を行う学習パラメータを取得（一旦、trainVarに取り分けてから格納）\n",
    "    g_training_parameter = [trainVar for trainVar in tf.trainable_variables()\n",
    "                          if 'GAN/generator/' in trainVar.name]\n",
    "    \n",
    "    # オプティマイザ(AdamOptimizer)にて学習パラメータの最適化を行う\n",
    "    # 一括取得したDiscriminatorのパラメータ更新\n",
    "    d_optimize = tf.train.AdamOptimizer(LEARNING_RATE).minimize(\n",
    "                d_loss, var_list=d_training_parameter)\n",
    "    # 一括取得したGeneratorのパラメータ更新\n",
    "    g_optimize = tf.train.AdamOptimizer(LEARNING_RATE).minimize(\n",
    "                g_loss, var_list=g_training_parameter)\n",
    "\n",
    "    batch = mnist.train.next_batch(BATCH_SIZE)\n",
    "\n",
    "    # 途中経過の保存する変数定義\n",
    "    save_gimage=[]\n",
    "    save_loss=[]\n",
    "    \n",
    "    counter=0\n",
    "    #（5）学習処理の実装\n",
    "    with tf.Session(config=config) as sess:\n",
    "      # 変数の初期化\n",
    "      sess.run(tf.global_variables_initializer()) \n",
    "\n",
    "      # EPOCHS数ぶん繰り返す\n",
    "      for e in range(EPOCHS):\n",
    "        # バッチサイズ100\n",
    "        for i in range(mnist.train.num_examples//BATCH_SIZE):\n",
    "          #####print('********{}/{} '.format(i, mnist.train.num_examples//BATCH_SIZE))\n",
    "          batch = mnist.train.next_batch(BATCH_SIZE)\n",
    "          batch_images = batch[0].reshape((BATCH_SIZE, 784))\n",
    "          #print(batch_images)  \n",
    "          # generatorにて活性化関数tanhを使用したためレンジを合わせる\n",
    "          batch_images = batch_images * 2 - 1\n",
    "          # generatorに渡す一様分布のランダムノイズを生成\n",
    "          # 値は-1〜1まで、サイズはbatch_size * 100\n",
    "          batch_z = np.random.uniform(-1,1,size=(BATCH_SIZE, 100))\n",
    "          # 最適化計算・パラメータ更新を行う\n",
    "          #####print('長さ batch={}'.format(len(batch_images)))\n",
    "          counter += 1\n",
    "          # Discriminatorの最適化に使うデータ群をfeed_dictで与える\n",
    "          sess.run(d_optimize, feed_dict = {ph_realData:batch_images, ph_randomData: batch_z})\n",
    "          # Generatorの最適化と最適化に使うデータ群をfeed_dictで与える\n",
    "          sess.run(g_optimize, feed_dict = {ph_randomData: batch_z})\n",
    " \n",
    "        # トレーニングのロスを記録\n",
    "        train_loss_d = sess.run(d_loss, {ph_randomData: batch_z, ph_realData:batch_images})\n",
    "        # evalはgのロス(g_loss)を出力する命令\n",
    "        train_loss_g = g_loss.eval({ph_randomData: batch_z})\n",
    "\n",
    "        # 学習過程の表示\n",
    "        print('{0} Epoch={1}/{2}, DLoss={3:.4F}, GLoss={4:.4F}'.format(\n",
    "              dt.datetime.now().strftime(\"%H:%M:%S\"),e+1,\n",
    "              EPOCHS,train_loss_d,train_loss_g))\n",
    "\n",
    "        # lossを格納するためのリストに追加する\n",
    "        # train_loss_d, train_loss_gをセットでリスト追加し、\n",
    "        # あとで可視化できるようにする\n",
    "        save_loss.append((train_loss_d, train_loss_g))\n",
    "\n",
    "        # 学習途中の生成モデルで画像を生成して保存する\n",
    "        # 一様性乱数データを25個生成して、そのデータを使って画像を生成し保存する。\n",
    "        randomData = np.random.uniform(-1, 1, size=(25, 100))\n",
    "        # gen_samplesに現時点のモデルで作ったデータを読ませておく\n",
    "        # ノイズ、サイズ、ユニット数(128)、reuseは状態保持、\n",
    "        # データはsample_zとしてfeed_dictに指定\n",
    "        gen_samples = sess.run(generator(ph_randomData, ALPHA, True),\n",
    "                              feed_dict={ph_randomData: randomData})\n",
    "        save_gimage.append(gen_samples)\n",
    "        \n",
    "    # pkl形式で生成画像を保存\n",
    "    with open('save_gimage.pkl', 'wb') as f:\n",
    "        pkl.dump(save_gimage, f)\n",
    "\n",
    "    # 各エポックで得た損失関数の値を保存\n",
    "    with open('save_loss.pkl', 'wb') as f:\n",
    "        pkl.dump(save_loss, f)\n",
    "\n",
    "    # 処理終了時刻の取得\n",
    "    tstamp_e = dt.datetime.now().strftime(\"%H:%M:%S\")\n",
    "    \n",
    "    time1 = dt.datetime.strptime(tstamp_s, \"%H:%M:%S\") \n",
    "    time2 = dt.datetime.strptime(tstamp_e, \"%H:%M:%S\") \n",
    "\n",
    "    # 処理時間を表示\n",
    "    print(\"開始: {0}、終了:{1}、処理時間:{2}\".format(tstamp_s, tstamp_e, (time2 - time1)))\n",
    "    print('counter={}'.format(counter))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
